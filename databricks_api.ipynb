{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e3b37ed-a54f-414b-8e58-73e7591b7020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ Data Engineer's Guide to API Data Retrieval & Error Handling\n",
    "\n",
    "## üìå Overview\n",
    "As a data engineer, your mission is clear: **extract, transform, and load (ETL) data from APIs** efficiently while ensuring robust error handling. APIs are powerful, but they come with challenges‚Äîtimeouts, rate limits, unexpected responses, and more. This notebook equips you with the tools to **fetch data reliably, handle errors gracefully, and log issues effectively**.\n",
    "\n",
    "## üîç What You'll Learn\n",
    "- How to **fetch data from APIs** using Python's `requests` library.\n",
    "- Implementing **try-except blocks** to catch and manage errors.\n",
    "- Setting up **logging** to track API failures and debugging issues.\n",
    "\n",
    "## üõ†Ô∏è Why This Matters\n",
    "Data pipelines depend on **consistent and reliable data ingestion**. Without proper error handling, a single failed request can disrupt workflows, leading to incomplete datasets or broken processes. By mastering API error handling, you ensure **data integrity, reliability, and efficiency** in your engineering tasks.\n",
    "\n",
    "---\n",
    "\n",
    "We are using the Application Programming Interface(API) from the United States Library of Congress: https://github.com/LibraryOfCongress\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ad9724d-3a7f-4138-93ae-3647e03d9d3f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "is Setting Up Logging and Data Parameters"
    }
   },
   "outputs": [],
   "source": [
    "import requests  # Library for making HTTP requests\n",
    "import json  # Library for parsing JSON data\n",
    "import logging  # Library for logging information\n",
    "import time  # Library for time-related functions\n",
    "from pyspark.sql import Row  # Class for creating Row objects\n",
    "from pyspark.sql.functions import to_date, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType  # Classes for defining DataFrame schema\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger('databricks_api_logging')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Define a formatter\n",
    "formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Define parameters for the API request and table\n",
    "state = \"Washington\"\n",
    "subject = \"Tacoma\"\n",
    "catalog = \"generaldata\"\n",
    "schema = \"dataanalysis\"\n",
    "table_name = \"tacoma_articles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20bab549-ad15-4c19-a4dc-8d735852acc3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fetching and Processing Newspaper Articles Data"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    for page in range(0, 10):\n",
    "        rows = []  # Initialize an empty list to store Row objects\n",
    "        # Make an initial API request to fetch data for the specified subject and state\n",
    "        response = requests.get(f\"http://chroniclingamerica.loc.gov/search/pages/results/?proxtext={subject}&state={state}&format=json\")\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        # Parse the JSON response from the initial API request\n",
    "        state_json = response.json()\n",
    "        json_keys = state_json['items'][0].keys()  # Extract keys for schema\n",
    "\n",
    "        df_schema = StructType([StructField(key, StringType(), True) for key in json_keys])  # Define schema\n",
    "   \n",
    "        response = requests.get(f\"http://chroniclingamerica.loc.gov/search/pages/results/?proxtext={subject}&state={state}&format=json&page={page+1}\")\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        article_data = json.loads(json.dumps(response.json()))\n",
    "        for article in article_data[\"items\"]:\n",
    "            rows.append(Row(**article))  # Append each article as a Row\n",
    "\n",
    "        df = spark.createDataFrame(rows, schema=df_schema)  # Create DataFrame with schema\n",
    "        df.write.mode(\"append\").saveAsTable(f\"{catalog}.{schema}.{table_name}\")  # Save DataFrame as a table\n",
    "        time.sleep(20)\n",
    "except requests.exceptions.HTTPError as err:\n",
    "    logger.error(f\"HTTP error occurred: {err}\")  # Log HTTP errors\n",
    "except requests.exceptions.RequestException as err:\n",
    "    logger.error(f\"Error occurred: {err}\")  # Log other request errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a233cf55-7870-4324-9969-f3d993322b84",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating a Summary of Text Using AI Model"
    }
   },
   "outputs": [],
   "source": [
    "# Read the table into a DataFrame\n",
    "df = spark.read.table(f\"{catalog}.{schema}.{table_name}\")\n",
    "\n",
    "# Select specific columns and generate summaries using AI query\n",
    "df_out = df.selectExpr(\n",
    "  \"date\",  # Select the date column\n",
    "  \"id\",  # Select the id column\n",
    "  \"subject\",  # Select the subject column\n",
    "  \"title\",  # Select the title column\n",
    "  \"ocr_eng\",  # Select the OCR text column\n",
    "  \"ai_query('databricks-meta-llama-3-3-70b-instruct', CONCAT('Please provide a summary of the following text: ', ocr_eng), named_struct('max_tokens', 100, 'temperature', 0.7)) as summary\"  # Generate summaries using AI model\n",
    ")\n",
    "df_out = df_out.withColumn(\"date\", to_date(col(\"date\"), \"yyyyMMdd\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99fd4a90-15a4-43f6-b58e-bf878ab0ad72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_out.take(5))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4798437006359821,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks_api",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
