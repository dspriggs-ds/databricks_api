{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e3b37ed-a54f-414b-8e58-73e7591b7020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ Data Engineer's Guide to API Data Retrieval & Error Handling\n",
    "\n",
    "## üìå Overview\n",
    "As a data engineer, your mission is clear: **extract, transform, and load (ETL) data from APIs** efficiently while ensuring robust error handling. APIs are powerful, but they come with challenges‚Äîtimeouts, rate limits, unexpected responses, and more. This notebook equips you with the tools to **fetch data reliably, handle errors gracefully, and log issues effectively**.\n",
    "\n",
    "## üîç What You'll Learn\n",
    "- How to **fetch data from APIs** using Python's `requests` library.\n",
    "- Implementing **try-except blocks** to catch and manage errors.\n",
    "- Setting up **logging** to track API failures and debugging issues.\n",
    "\n",
    "## üõ†Ô∏è Why This Matters\n",
    "Data pipelines depend on **consistent and reliable data ingestion**. Without proper error handling, a single failed request can disrupt workflows, leading to incomplete datasets or broken processes. By mastering API error handling, you ensure **data integrity, reliability, and efficiency** in your engineering tasks.\n",
    "\n",
    "---\n",
    "\n",
    "We are using the Application Programming Interface(API) from the United States Library of Congress: https://github.com/LibraryOfCongress\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ad9724d-3a7f-4138-93ae-3647e03d9d3f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "is Setting Up Logging and Data Parameters"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import logging\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger('databricks_api_logging')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Define a formatter\n",
    "formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Define parameters for the API request and table\n",
    "state = \"Washington\"\n",
    "subject = \"Tacoma\"\n",
    "catalog = \"generaldata\"\n",
    "schema = \"dataanalysis\"\n",
    "table_name = \"tacoma_articles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20bab549-ad15-4c19-a4dc-8d735852acc3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fetching and Processing Newspaper Articles Data"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    rows = []\n",
    "    response = requests.get(f\"http://chroniclingamerica.loc.gov/search/pages/results/?proxtext={subject}&state={state}&format=json\")\n",
    "    response.raise_for_status()  # Raise an error for bad status codes\n",
    "    # Parse the JSON response from the initial API request\n",
    "    state_json = response.json()\n",
    "    # Define the number of pages to fetch\n",
    "    numPages = 50\n",
    "    json_keys = state_json['items'][0].keys()  # Extract keys for schema\n",
    "\n",
    "    df_schema = StructType([StructField(key, StringType(), True) for key in json_keys])  # Define schema\n",
    "\n",
    "    for p in range(0, numPages):\n",
    "        response = requests.get(f\"http://chroniclingamerica.loc.gov/search/pages/results/?proxtext={subject}&state={state}&format=json&page={p+1}\")\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        article_data = json.loads(json.dumps(response.json()))\n",
    "        for article in article_data[\"items\"]:\n",
    "            rows.append(Row(**article))  # Append each article as a Row\n",
    "\n",
    "    df = spark.createDataFrame(rows, schema=df_schema)  # Create DataFrame with schema\n",
    "    df.write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{subject}\")  # Save DataFrame as a table\n",
    "except requests.exceptions.HTTPError as err:\n",
    "    logger.error(f\"HTTP error occurred: {err}\")  # Log HTTP errors\n",
    "except requests.exceptions.RequestException as err:\n",
    "    logger.error(f\"Error occurred: {err}\")  # Log other request errors"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks_api",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
